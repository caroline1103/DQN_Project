{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroline1103/DQN_Project/blob/main/src/DQN_on_CARTPOLE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#DQN ON CARTPOLE**"
      ],
      "metadata": {
        "id": "DxZtRhnx3OPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Set up"
      ],
      "metadata": {
        "id": "4Jn3hsF13Sv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installations"
      ],
      "metadata": {
        "id": "DUHKd2bG3XQa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C5f3TLJj0tKa",
        "outputId": "dd7d8a2b-b028-4144-e534-0cf6b9c5c1d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (5.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.21.6)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (2.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.11.0)\n"
          ]
        }
      ],
      "source": [
        "pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYTVjbQj0tKS"
      },
      "source": [
        "\n",
        "##Packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sKHaFBRK0tKe"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple, deque #to construct transitions and replay memory\n",
        "from itertools import count \n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display #this is create animation\n",
        "plt.ion()\n",
        "\n",
        "import gym #environment to play the games\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn #constructing neural network\n",
        "import torch.optim as optim #optimizing\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#For the videogame animation at the end\n",
        "import os\n",
        "import sys\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "import pygame\n",
        "pygame.init()\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Transitions and their storage in a replay memory\n"
      ],
      "metadata": {
        "id": "7tXGSeNZZ04L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the beggining, we want to let the computer explore freely the environment by making mistakes and seeing in what situations it gets rewarded. However, when it has had enough experiences, we also want it to start learning from what happened.\n",
        "\n",
        "For this purpose, we store those events as transitions between two states.\n",
        "\n",
        "In this example, we learn randomly from transitions by giving them all the same importance. However, the batches could be created differently.\n",
        "\n",
        "Here, we train **off-policy**, because at each update we use any transition in the replay memeort, regardless of how the agent was choosing to explore the environment when the data was obtained."
      ],
      "metadata": {
        "id": "cncjjU5DEIWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))"
      ],
      "metadata": {
        "id": "fqLnS9COZP1n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a replay memory where we will store all the transitions up to a certain capacity (then we will overwrite over the oldest ones)."
      ],
      "metadata": {
        "id": "HYStN4iPEOdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replay_memory(capacity = 20000):\n",
        "  return deque([],maxlen = capacity)"
      ],
      "metadata": {
        "id": "5UuC0SP3ZQcV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_to_memory(memory, transition):\n",
        "  memory.append(transition[:])"
      ],
      "metadata": {
        "id": "UKgtKYsBZQfL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will want to get a random batch of memory to learn from later one and we'll use this function:"
      ],
      "metadata": {
        "id": "OKVYyC9r36X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def meomry_batch(memory, batch_size = 128):\n",
        "  return random.sample(memory, batch_size)"
      ],
      "metadata": {
        "id": "LvkwTlydZlWW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up the cartpole environment"
      ],
      "metadata": {
        "id": "m-vSY4NhaAQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we want to learn to play the cartpole game.\n",
        "\n",
        "**Goal :** The goal of the game is to balance a pole on a cart which is moving left and right. We lose when the pole falls. An episode ends when the pole is more than 15 degrees from vertical or when the cart moved more than 2.4 units from the center.\n",
        "\n",
        "**Rewards :** You score points by staying upright. The computer gains one point after each action where the pole stays upright.\n",
        "\n",
        "**Observations:** We observe a vector of 4 parameters: the position of the cart, the velocity of the cart, the angle of the pole, and the rotation rate of the pole.\n",
        "\n",
        "**Actions:** We can only take two actions: moving the cart right or moving it left.\n"
      ],
      "metadata": {
        "id": "dmE-7m42EVmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 2\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "env = gym.make('CartPole-v1', new_step_api=True)\n",
        "env.reset(seed=RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n"
      ],
      "metadata": {
        "id": "3O-wjhB2ZqVT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Action Space: {}\".format(env.action_space))\n",
        "\n",
        "print(\"State space: {}\".format(env.observation_space)) #[position of cart, velocity of cart, angle of pole, rotation rate of pole]"
      ],
      "metadata": {
        "id": "1twlG65dE4YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0155b2e6-c1d2-4cbb-8f2e-81e5d7fbd60a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(2)\n",
            "State space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we **take an action** in the environment, we **get back four informations**: \n",
        "* The new state of the environment after the action\n",
        "* The reward we got\n",
        "* If the game was terminated (we lost) or truncated (the cart went out of the frame)\n",
        "* Additional informations."
      ],
      "metadata": {
        "id": "qhtMIDPZ6MZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(1) #next_state, reward, terminated, truncated , info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He2FN3yzaYE4",
        "outputId": "b7e2ee1c-f898-49c5-af93-a4c3cb328124"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.02424181,  0.17450671,  0.03060641, -0.32341394], dtype=float32),\n",
              " 1.0,\n",
              " False,\n",
              " False,\n",
              " {})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As seen earlier, we observe the environment of cartpole with **4 informations** and we can take **2 actions**:\n"
      ],
      "metadata": {
        "id": "YX3M0rlpPoo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "\n",
        "n_observations = len(state) #for cartpole it's 4\n",
        "n_actions = env.action_space.n #for carpole it's 2\n"
      ],
      "metadata": {
        "id": "6ob-ruX_DftZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Wdk5TP0tKm"
      },
      "source": [
        "#DQN, the action-value function Q\n",
        "\n",
        "In summary, the goal of our algorithm is to find the optimal action that the agent should take at each step. \n",
        "\n",
        "This is when the action value function $Q*(s,a)$ comes in. It returns the maximum expected return achievable from taking the action $a$ after observing a state $s$.\n",
        "\n",
        "We define the action value function as below with the **Bellman equation**:\n",
        "\n",
        "$Q*(s,a) = r + \\gamma max Q*(s',a')$ ,\n",
        "\n",
        "with $r$ the reward in we get from taking action $a$ in state $s$, $Q*(s',a')$ is the expected future return we can get from the next state and $\\gamma$ is the discount factor (as we would rather be rewarded sooner than later).\n",
        "\n",
        "But in reality, we don't know $Q*$ and we need to approximate it. To this purpose we use a neural network which we define as follows(same as implemented on [the pytorch website](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jJL6rVx_0tKo"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Selecting an action\n",
        "\n",
        "An import step is to actually select an action for the computer to execute. From the DQN we defined above, this should be pretty easy.\n",
        "\n",
        "However, we need to consider that at the beggining of the training, the computer doesn't know anything: it has no memories.\n",
        "\n",
        "Furthermore, we want it to keep exploring other possibilites and avoid it taking always the same decisions which could return only a small reward (this cannot happen with carpole as the actions and reward are particularly limited but it will be the case in more complex games).\n",
        "\n",
        "To avoid this issue, we take an $ϵ$ between 0 and 1 such that the agent takes a random action $ϵ$% of the time, and takes an action which maximes $Q*(s,a)$  the rest of the time. \n",
        "\n",
        "We will see later on that we define \\epsilon such that it starts at one (the computer only explores new things at the beginning as it has no memories) and slowly and linearly decreases to a $\\epsilon_{minimum}$ which is greater than 0. As time passes, the computer uses more and more its memory to chose its actions, but still explores a little as to possibly further improve its score.\n"
      ],
      "metadata": {
        "id": "RjJq0eKvcpaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state, eps_threshold):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold: #we choose an action from what we learned\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "            #this returns the index (action) which allows to get maximum rewards\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], dtype=torch.long) #we choose a random action\n",
        "\n"
      ],
      "metadata": {
        "id": "9KoIHig9cofu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating a plot of progress\n",
        "\n",
        "I kept the function from the pytorch tutorial as it allows for a quick overview of the progress of the computer.\n",
        "\n",
        "The plot duration is the number of actions the computer was able to take before losing. \n",
        "In cartpole, there is basically no delays so it's also the number of actions the computer was able to take without losing."
      ],
      "metadata": {
        "id": "SWxCc7bT9_2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_durations(show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "\n",
        "    #at the end\n",
        "    if show_result: \n",
        "        plt.title('Result')\n",
        "\n",
        "    #during\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "        \n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "\n",
        "    # After we went trough 100 episodes, we start ploting the average duration\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    \n",
        "    if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)"
      ],
      "metadata": {
        "id": "79c8FBccgKjW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining our DQN networks\n",
        "\n",
        "One thing we didn't mention yet is that we are going to need two networks. Reinforcement learning tends to be quite unstable and can diverge in some cases. Some reasons might be that one action can have a huge impact on the learning curve or that there exists correlations between past and present states.\n",
        "\n",
        "We can call them the **policy network** and the **target network**.\n",
        "\n",
        "When we want to optimize our model, we need to perform a gradient descent. In this situation, we use the policy network to estimate $Q_{predicted}$ for each actions. We want to compare those predicted values to the ones we want, let's call them $Q_{targeted}$ with a loss function (the Huber loss function)\n",
        "\n",
        "Basically, we are going to update the policy network at every step during the gradient descent, and we will do a soft update at a rate to be determine on the target network with the policy network weights.\n",
        "\n",
        "(Another thing we could do is to update the weights only each $x$ steps.)\n"
      ],
      "metadata": {
        "id": "ZcFJUAACEjaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = DQN(n_observations, n_actions)\n",
        "target_net = DQN(n_observations, n_actions)\n",
        "target_net.load_state_dict(policy_net.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MwTUF5nEfVs",
        "outputId": "d441e9ad-82fb-434b-b366-6c6cdc0c0977"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A few parameters to define"
      ],
      "metadata": {
        "id": "HdW_UrUQDp8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to define **a few parameters** before we start training our algorithm:\n",
        "\n",
        "* As mentionned in the DQN function, $\\gamma$ is the **discount factor** so that futur reward are less meaningful than direct rewards.\n",
        "\n",
        "* As mentionned in the select_action function, we need **epsilon** to slowly decrease at **rate $\\epsilon_{decay}$** from $\\epsilon_{maximum}$ to $\\epsilon_{minimum}$. The higher $\\epsilon_{decay}$ is the lower the decay will be when we define $\\epsilon$ as we will see later on.\n",
        "\n",
        "* We also need to define at which **rate $τ$ we upadte the weights of the target network**. \n",
        "\n",
        "* The **Adam optimizer** and its **learning rate** LR. This optimizer is prefered in the litterature to the classical stochastic gradient descent as it's supposed to be more efficient and particularly adapted to these types of problems."
      ],
      "metadata": {
        "id": "Y4daxLaIDdBG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EfOUINlR0tKs"
      },
      "outputs": [],
      "source": [
        "\n",
        "GAMMA = 0.99 # Discount factor\n",
        "\n",
        "E_max = 0.9 # Starting epsilon\n",
        "E_min = 0.05 # Ending epsilon\n",
        "E_decay = 1000 # Rate of decrease\n",
        "\n",
        "TAU = 0.005 # Rate at which we want to update the weights from the policy to the target nn\n",
        "\n",
        "LR = 1e-4 # Learning rate of the AdamW optimizer\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizing the model\n",
        "\n",
        "Once we have enough memory to get a random sample out of them, we start the optimization by creating a memory batch of transitions.\n",
        "\n",
        "For all state and action, we compute their Q-function values through the policy network and then we compute the expected future rewards. We then compare the expected future rewards with the actual rewards we got from taking an action and we compute the Huber loss.\n",
        "\n",
        "If the state is terminal, we set the reward to be 0 by default.\n",
        "\n",
        "We then use the Huber loss in our backward propagation as to update our weights.\n"
      ],
      "metadata": {
        "id": "5vgUUZwBKGIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QrZhA7Tw0tKw"
      },
      "outputs": [],
      "source": [
        "def optimize_model(BATCH_SIZE = 128):\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    transitions = meomry_batch(memory, BATCH_SIZE)\n",
        "\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    #Non final states are states which do not lead to the game stoping\n",
        "\n",
        "    #We get all those transitions\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)), dtype=torch.bool)\n",
        "    #And all of those states\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        #For the non final states, we find the actions we should take through our target nn\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "\n",
        "    # We apply the Bellman equation to get the expected rewards at each states\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss between what we expected the reward to be for an action and what the reward actually was\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model with backward propagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D-_SsVP0tKy"
      },
      "source": [
        "Below, you can find the main training loop. At the beginning we reset\n",
        "the environment and obtain the initial ``state`` Tensor. Then, we sample\n",
        "an action, execute it, observe the next state and the reward (always\n",
        "1), and optimize our model once. When the episode ends (our model\n",
        "fails), we restart the loop.\n",
        "\n",
        "Below, `num_episodes` is set to 600 if a GPU is available, otherwise 50 \n",
        "episodes are scheduled so training does not take too long. However, 50 \n",
        "episodes is insufficient for to observe good performance on cartpole.\n",
        "You should see the model constantly achieve 500 steps within 600 training \n",
        "episodes. Training RL agents can be a noisy process, so restarting training\n",
        "can produce better results if convergence is not observed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "aDyIbIN-PWo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting up a few variables before the main training loop"
      ],
      "metadata": {
        "id": "rWgOJP6sPRZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = replay_memory(10000) #creating the replay memory of size 10000\n",
        "\n",
        "steps_done = 0 #setting steps at 0 at beginning\n",
        "\n",
        "episode_durations = [] #Where we store the episodes durations for the plots\n",
        "\n",
        "num_episodes = 600 #number of games we want to play"
      ],
      "metadata": {
        "id": "xHzdh-oqIHfR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The main training loop"
      ],
      "metadata": {
        "id": "BBcjmT0cQF98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5V1-YhBI0tKz",
        "outputId": "0349d209-bbd1-404e-adf2-97ef33e25d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#For the number of games we planned to train \n",
        "for i_episode in range(num_episodes):\n",
        "\n",
        "    # We initialize the state of the environment\n",
        "    state = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    #In each game\n",
        "    for t in count():\n",
        "\n",
        "        #We update epsilon as we defined previously\n",
        "        eps_threshold = E_min + (E_max - E_min) * math.exp(-1. * steps_done / E_decay)\n",
        "\n",
        "        #We select an action from the state and the current epsilon\n",
        "        #It's either a random action or one from our DQN\n",
        "        action = select_action(state, eps_threshold)\n",
        "\n",
        "        #We do the action we chose\n",
        "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
        "\n",
        "        #We get a reward\n",
        "        reward = torch.tensor([reward])\n",
        "\n",
        "        #We see if we lost\n",
        "        done = terminated or truncated\n",
        "\n",
        "        #If we lost, there is no next state but we still add it to the memory\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "\n",
        "        #If we didin't lose we get the new state we are in\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        #Either way, we store the transition in our memory\n",
        "        add_to_memory(memory, (state, action, reward, next_state, done))\n",
        "\n",
        "        #And our new state is now our present state\n",
        "        state = next_state\n",
        "\n",
        "        #We optimize the weights of our model on the policy network\n",
        "        optimize_model()\n",
        "\n",
        "        #And we do a soft update on the ones of the target network\n",
        "        #With the ones from our policy network\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        #If we lost \n",
        "        if done:\n",
        "            #We plot the duration of the game\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "#After we completed all the training games, we show the complete durations plot\n",
        "print('Complete')\n",
        "plot_durations(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testitng and displaying the game played"
      ],
      "metadata": {
        "id": "jUwVceDpgjhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before training\n",
        "\n",
        "For illustration purpose, it's nice to visualize our game.\n",
        "\n",
        "Here, we plot how the computer plays before ot was trained, meaning it's taking random actions.\n",
        "\n",
        "We just create an array which stores RGB images of the game until it's over. The computer takes an action and updates it's current state until the game is over.\n",
        "\n",
        "Then we can animate all those images but putting them one after another."
      ],
      "metadata": {
        "id": "D6pOCwt7go27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "observation = env.reset()\n",
        "\n",
        "ims =[]\n",
        "done = False\n",
        "while not done:   \n",
        "    im = plt.imshow(env.render(mode = 'rgb_array'), animated=True)\n",
        "    ims.append([im])\n",
        "\n",
        "    action = env.action_space.sample()\n",
        "    new_observation, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    observation = new_observation\n",
        "ims.append([plt.imshow(env.render(mode = 'rgb_array'), animated=True)])\n",
        "\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
        "                                repeat_delay=100)\n",
        "\n",
        "ani.save('Cartpole_before_training.mp4')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "LfkpxbbtgTpo",
        "outputId": "cbe37693-5f3d-4ae9-f94c-3cc5ca730c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##After Training\n",
        "\n",
        "We do the same thing except we don't take random actions we take an action from the policy network."
      ],
      "metadata": {
        "id": "p4Dm7HiJhJIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "state = env.reset()\n",
        "state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "ims =[]\n",
        "done = False\n",
        "while not done:   \n",
        "    im = plt.imshow(env.render(mode = 'rgb_array'), animated=True)\n",
        "    ims.append([im])\n",
        "\n",
        "    #By setting epsilon to be 0 we are sure to never take a random action\n",
        "    action = select_action(state,0)\n",
        "    new_state, reward, terminated, truncated, info = env.step(action.item())\n",
        "    state = new_state\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    done = terminated or truncated\n",
        "ims.append([plt.imshow(env.render(mode = 'rgb_array'), animated=True)])\n",
        "\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
        "                                repeat_delay=100)\n",
        "\n",
        "ani.save('Cartpole_after_training.mp4')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "33lsW59Hg1Sj",
        "outputId": "05999c83-d798-48a5-b86c-6e77298159d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}