{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroline1103/DQN_Project/blob/main/src/DQN_on_BREAKOUT_unfinished.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DQN ON BREAKOUT**\n",
        "\n",
        "The unfinished notebook which presents a few functions which should have improved the results had they worked.\n",
        "\n",
        "The added function are:\n",
        "* A frame processor as to get greyscale square images\n",
        "* A different DQN neural network as seen in litterature\n",
        "* A state is now 4 frames stacked together as we need to see in which direction the ball moves\n",
        "\n"
      ],
      "metadata": {
        "id": "kzDKZ_vv6Wnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up\n",
        "\n"
      ],
      "metadata": {
        "id": "tNFGSMXW0pjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installations"
      ],
      "metadata": {
        "id": "0GyQsj_L04xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pynput"
      ],
      "metadata": {
        "id": "o_VsLM_UH4Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pygame"
      ],
      "metadata": {
        "id": "42f2412Inx8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym[atari,accept-rom-license]==0.21.0"
      ],
      "metadata": {
        "id": "T3loyIrilDsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5f3TLJj0tKa"
      },
      "outputs": [],
      "source": [
        "pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "cgD1x6z1lk9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Packages\n"
      ],
      "metadata": {
        "id": "KU6sGShykn8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKHaFBRK0tKe"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display #for the animations\n",
        "plt.ion()\n",
        "\n",
        "import gym\n",
        "\n",
        "#for the neural network\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import huber_loss\n",
        "#device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "from skimage.transform import resize"
      ],
      "metadata": {
        "id": "U2Y6PXkiE5FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transitions and their storage in a replay memory\n"
      ],
      "metadata": {
        "id": "7tXGSeNZZ04L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the beggining, we want to let the computer explore freely the environment by making mistakes and seeing in what situations it gets rewarded. However, when it has had enough experiences, we also want it to start learning from what happened.\n",
        "\n",
        "For this purpose, we store those events as transitions between two states.\n",
        "\n",
        "In this example, we learn randomly from transitions by giving them all the same importance. However, the batches could be created differently.\n",
        "\n",
        "Here, we train **off-policy**, because at each update we use any transition in the replay memeort, regardless of how the agent was choosing to explore the environment when the data was obtained."
      ],
      "metadata": {
        "id": "uhiIH0q-6Th_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))"
      ],
      "metadata": {
        "id": "fqLnS9COZP1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a replay memory where we will store all the transitions up to a certain capacity (then we will overwrite over the oldest ones)."
      ],
      "metadata": {
        "id": "4D_p7vDH-snC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replay_memory(capacity = 20000):\n",
        "  return deque([],maxlen = capacity)"
      ],
      "metadata": {
        "id": "5UuC0SP3ZQcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_to_memory(memory, transition):\n",
        "  memory.append(transition[:])"
      ],
      "metadata": {
        "id": "UKgtKYsBZQfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will want to get a random batch of memory to learn from later one and we'll use this function:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n-0yWYLO--4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def meomry_batch(memory, batch_size = 128):\n",
        "  return random.sample(memory, batch_size)"
      ],
      "metadata": {
        "id": "LvkwTlydZlWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up the Breakout environment\n",
        "\n",
        "**Goal :** The goal of the game is to destroy the wall at the top of the screen by hitting the ball with a paddle. You have five lifes before losing a game.\n",
        "\n",
        "**Rewards :** You score points by destroying bricks in the wall. The reward for destroying a brick depends on the color of the brick.\n",
        "\n",
        "**Observations:** We observe a full image of the game, by default in RGB.\n",
        "\n",
        "**Actions:** We choose difficulty 0 for maximum simplicity. Then, we have four actions available: move left or right, do nothing and fire.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8MCbZIKYkTxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Breakout-v4\", difficulty = 0)\n",
        "\n",
        "env.reset(seed = 1)\n",
        "\n",
        "print(\"Action Space: {}\".format(env.action_space)) \n",
        "#['NOOP', 'FIRE', 'RIGHT', 'LEFT'] = [0,1, 2,3]\n",
        "\n",
        "print(\"State space: {}\".format(env.observation_space))\n",
        "#The RGB image\n",
        "\n",
        "print(\"Step output:\",env.step(1)) #returns: next_state, reward, terminated, truncated , info\n"
      ],
      "metadata": {
        "id": "WgFGq-okl8Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 2\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "env = gym.make(\"Breakout-v4\", difficulty = 0)\n",
        "env.reset(seed=RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n"
      ],
      "metadata": {
        "id": "3O-wjhB2ZqVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforming the images in tensorflow"
      ],
      "metadata": {
        "id": "KixnqWCvyHOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As to optimize our algorithm, we need to redefine image in greyscale and cropping them.\n",
        "\n",
        "(**The function doesn't work properly as tf.placeholder needs an older version of tenserflow to work, but then I did not manage to get back a numpy array.**)"
      ],
      "metadata": {
        "id": "VrkfNsqwFrre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "def FrameProcessor(frame, frame_height=84, frame_width=84):\n",
        "    \"\"\"Resizes and converts RGB Atari frames to grayscale\"\"\"\n",
        "        #Args:\n",
        "        #    frame_height: Integer, Height of a frame of an Atari game\n",
        "          #  frame_width: Integer, Width of a frame of an Atari game\n",
        "    frame_height = frame_height\n",
        "    frame_width = frame_width\n",
        "    frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
        "    processed = tf.image.rgb_to_grayscale(frame)\n",
        "    processed = tf.image.crop_to_bounding_box(processed, 34, 0, 160, 160)\n",
        "    processed = tf.image.resize_images(processed, \n",
        "                                                [frame_height, frame_width], \n",
        "                                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    \n",
        "    return processed"
      ],
      "metadata": {
        "id": "riSef-0KFfs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Wdk5TP0tKm"
      },
      "source": [
        "#DQN, the action-value function Q\n",
        "\n",
        "In summary, the goal of our algorithm is to find the optimal action that the agent should take at each step. \n",
        "\n",
        "This is when the action value function $Q*(s,a)$ comes in. It returns the maximum expected return achievable from taking the action $a$ after observing a state $s$.\n",
        "\n",
        "We define the action value function as below with the **Bellman equation**:\n",
        "\n",
        "$Q*(s,a) = r + \\gamma max Q*(s',a')$ ,\n",
        "\n",
        "with $r$ the reward in we get from taking action $a$ in state $s$, $Q*(s',a')$ is the expected future return we can get from the next state and $\\gamma$ is the discount factor (as we would rather be rewarded sooner than later).\n",
        "\n",
        "But in reality, we don't know $Q*$ and we need to approximate it. To this purpose we use a neural network which we define as follows:\n",
        "\n",
        "**It's the basic algorithm as defined in Mnih et al. 2013, but also more largely in most litteratures about the subject. The input should be four stacked frames which then define a state.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJL6rVx_0tKo"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_actions, agent_history_length = 4):\n",
        "\n",
        "        super(DQN, self).__init__()\n",
        "        self.__conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, bias=False)\n",
        "        self.__conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, bias=False)\n",
        "        self.__conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, bias=False)\n",
        "        self.__fc1 = nn.Linear(1280, 512)\n",
        "        self.__fc2 = nn.Linear(512, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.__conv1(x))\n",
        "        x = F.relu(self.__conv2(x))\n",
        "        x = F.relu(self.__conv3(x))\n",
        "        x = F.relu(self.__fc1(x.view(x.size(0), -1)))\n",
        "        return self.__fc2(x)\n",
        "\n",
        "    def init_weights(module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity=\"relu\")\n",
        "            module.bias.data.fill_(0.0)\n",
        "        elif isinstance(module, nn.Conv2d):\n",
        "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity=\"relu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Selecting an action\n",
        "\n",
        "An import step is to actually select an action for the computer to execute. From the DQN we defined above, this should be pretty easy.\n",
        "\n",
        "However, we need to consider that at the beggining of the training, the computer doesn't know anything: it has no memories.\n",
        "\n",
        "Furthermore, we want it to keep exploring other possibilites and avoid it taking always the same decisions which could return only a small reward (this cannot happen with carpole as the actions and reward are particularly limited but it will be the case in more complex games).\n",
        "\n",
        "To avoid this issue, we take an $ϵ$ between 0 and 1 such that the agent takes a random action $ϵ$% of the time, and takes an action which maximes $Q*(s,a)$  the rest of the time. \n",
        "\n",
        "We will see later on that we define \\epsilon such that it starts at one (the computer only explores new things at the beginning as it has no memories) and slowly and linearly decreases to a $\\epsilon_{minimum}$ which is greater than 0. As time passes, the computer uses more and more its memory to chose its actions, but still explores a little as to possibly further improve its score.\n",
        "\n"
      ],
      "metadata": {
        "id": "RjJq0eKvcpaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state, eps_threshold, step):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold and step == 4:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], dtype=torch.long, device=device)\n",
        "\n"
      ],
      "metadata": {
        "id": "9KoIHig9cofu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating a plot of progress\n",
        "\n",
        "I kept the function from the pytorch tutorial as it allows for a quick overview of the progress of the computer.\n",
        "\n",
        "The plot duration is the number of actions the computer was able to take before losing. \n",
        "In cartpole, there is basically no delays so it's also the number of actions the computer was able to take without losing."
      ],
      "metadata": {
        "id": "a0yaj4qszW_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_durations(show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    \n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ],
      "metadata": {
        "id": "79c8FBccgKjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93j6jFzb0tKr"
      },
      "source": [
        "#Defining our DQN networks\n",
        "\n",
        "One thing we didn't mention yet is that we are going to need two networks. Reinforcement learning tends to be quite unstable and can diverge in some cases. Some reasons might be that one action can have a huge impact on the learning curve or that there exists correlations between past and present states.\n",
        "\n",
        "We can call them the **policy network** and the **target network**.\n",
        "\n",
        "When we want to optimize our model, we need to perform a gradient descent. In this situation, we use the policy network to estimate $Q_{predicted}$ for each actions. We want to compare those predicted values to the ones we want, let's call them $Q_{targeted}$ with a loss function (the Huber loss function)\n",
        "\n",
        "Basically, we are going to update the policy network at every step during the gradient descent, and we will do a soft update at a rate to be determine on the target network with the policy network weights.\n",
        "\n",
        "(Another thing we could do is to update the weights only each $x$ steps.)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "policy_net.apply(DQN.init_weights).to(device)\n",
        "target_net = DQN(n_observations, n_actions)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n"
      ],
      "metadata": {
        "id": "97ljr5bW0IHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A few parameters to define\n",
        "\n",
        "We need to define **a few parameters** before we start training our algorithm:\n",
        "\n",
        "* As mentionned in the DQN function, $\\gamma$ is the **discount factor** so that futur reward are less meaningful than direct rewards.\n",
        "\n",
        "* As mentionned in the select_action function, we need **epsilon** to slowly decrease at **rate $\\epsilon_{decay}$** from $\\epsilon_{maximum}$ to $\\epsilon_{minimum}$. The higher $\\epsilon_{decay}$ is the lower the decay will be when we define $\\epsilon$ as we will see later on.\n",
        "\n",
        "* We also need to define at which **rate $τ$ we upadte the weights of the target network**. \n",
        "\n",
        "* The **Adam optimizer** and its **learning rate** LR. This optimizer is prefered in the litterature to the classical stochastic gradient descent as it's supposed to be more efficient and particularly adapted to these types of problems."
      ],
      "metadata": {
        "id": "7A0WJehv0Iol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfOUINlR0tKs"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "GAMMA = 0.99 # Discount factor\n",
        "\n",
        "E_max = 0.9 # Starting epsilon\n",
        "E_min = 0.05 # Ending epsilon\n",
        "E_decay = 1000 # Rate of decrease\n",
        "\n",
        "TAU = 0.005 # Rate at which we want to update the weights from the policy to the target nn\n",
        "\n",
        "LR = 1e-4 # Learning rate of the AdamW optimizer\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizing the model\n",
        "\n",
        "Once we have enough memory to get a random sample out of them, we start the optimization by creating a memory batch of transitions.\n",
        "\n",
        "For all state and action, we compute their Q-function values through the policy network and then we compute the expected future rewards. We then compare the expected future rewards with the actual rewards we got from taking an action and we compute the Huber loss.\n",
        "\n",
        "If the state is terminal, we set the reward to be 0 by default.\n",
        "\n",
        "We then use the Huber loss in our backward propagation as to update our weights."
      ],
      "metadata": {
        "id": "XW_MjtvY0zDt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrZhA7Tw0tKw"
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = meomry_batch(memory, BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)), dtype=torch.bool, device=device)\n",
        "\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.functional.huber_loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training\n"
      ],
      "metadata": {
        "id": "JBL49VXM1B97"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg6LV-8x0tKt"
      },
      "source": [
        "##Setting up a few variables before the main training loop\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of state observations\n",
        "state = env.reset()\n",
        "\n",
        "n_observations = len(state) #for breakout 210\n",
        " \n",
        "n_actions = env.action_space.n #for breakout 4"
      ],
      "metadata": {
        "id": "bIR8JrD20k8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = replay_memory(10000) #creating the replay memory of size 10000\n",
        "\n",
        "steps_done = 0 #setting steps at 0 at beginning\n",
        "\n",
        "episode_durations = [] #Where we store the episodes durations for the plots\n",
        "\n",
        "num_episodes = 600 #number of games we want to play"
      ],
      "metadata": {
        "id": "GoHpu__wpfrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Huber loss as it is the loss function recommanded. It shoud prevent the exploding gradient problem from happening. This comes from the fact that in recurrent networks, the error gradients can accumate during the updates and become very large. It can result in unstability in the nerwork."
      ],
      "metadata": {
        "id": "Z26GBUgpKLPq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V1-YhBI0tKz"
      },
      "outputs": [],
      "source": [
        "num_episodes = 600\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and get it's state\n",
        "    state = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) #put in select next state\n",
        "    frame = 1\n",
        "    full_state = []\n",
        "    for t in count():\n",
        "\n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "        action = select_action(full_state, eps_threshold, frame)\n",
        "\n",
        "        observation, reward, done, info = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        if done:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        full_state.append((state, action, reward, next_state, done))\n",
        "        if frame == 4:\n",
        "          # Store the transition in memory\n",
        "          add_to_memory(memory, full_state)\n",
        "          frame = 1\n",
        "          full_state = []\n",
        "        else:\n",
        "          frame += 1\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        # θ′ ← τ θ + (1 −τ )θ′\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "\n",
        "print('Complete')\n",
        "plot_durations(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mzynVbqjnTXO"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}